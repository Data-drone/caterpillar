# caterpillar: Tools to tokenize text
#
# Copyright (C) 2012-2013 Mammoth Labs
# Author: Ryan Stuart <ryan@mammothlabs.com.au>

from nltk.internals import convert_regexp_to_nongrouping
from nltk.tokenize.api import TokenizerI
from nltk.tokenize.util import regexp_span_tokenize

import regex


class NewRegexpTokenizer(TokenizerI):
    """
    A tokenizer that splits a string using a regular expression.

    This class can be used to match either the tokens or the separators between tokens.

        >>> tokenizer = NewRegexpTokenizer('\w+|\$[\d\.]+|\S+')

    This class is basically a copy of ''ntlk.RegexpRokenizer'' except that we use the new python regex module instead
    of the existing re module. This means unicode codepoint properties are supported.

        >>> tokenizer = NewRegexpTokenizer('\w+|\$[\p{N}\.]+|\S+')

    :type pattern: str
    :param pattern: The pattern used to build this tokenizer. (This pattern may safely contain grouping parentheses.)
    :type gaps: bool
    :param gaps: True if this tokenizer's pattern should be used to find separators between tokens; False if this
        tokenizer's pattern should be used to find the tokens themselves.
    :type discard_empty: bool
    :param discard_empty: True if any empty tokens `''` generated by the tokenizer should be discarded.  Empty
        tokens can only be generated if `_gaps == True`.
    :type flags: int
    :param flags: The regexp flags used to compile this tokenizer's pattern.  By default, the following flags are
        used: `regex.UNICODE | regex.MULTILINE | regex.DOTALL | regex.VERSION1`.

    """
    def __init__(self, pattern, gaps=False, discard_empty=True,
                 flags=regex.UNICODE | regex.MULTILINE | regex.DOTALL | regex.VERSION1):
        # If they gave us a regexp object, extract the pattern.
        pattern = getattr(pattern, 'pattern', pattern)

        self._pattern = pattern
        self._gaps = gaps
        self._discard_empty = discard_empty
        self._flags = flags
        self._regexp = None

        # Remove grouping parentheses -- if the regexp contains any
        # grouping parentheses, then the behavior of re.findall and
        # re.split will change.
        nongrouping_pattern = convert_regexp_to_nongrouping(pattern)

        try:
            self._regexp = regex.compile(nongrouping_pattern, flags)
        except regex.error, e:
            raise ValueError('Error in regular expression {}: {}'.format(pattern, e))

    def tokenize(self, text):
        # If our regexp matches gaps, use re.split:
        if self._gaps:
            if self._discard_empty:
                return [tok for tok in self._regexp.split(text) if tok]
            else:
                return self._regexp.split(text)

        # If our regexp matches tokens, use re.findall:
        else:
            return self._regexp.findall(text)

    def span_tokenize(self, text):
        if self._gaps:
            for left, right in regexp_span_tokenize(text, self._regexp):
                if not (self._discard_empty and left == right):
                    yield left, right
        else:
            for m in regex.finditer(self._regexp, text):
                yield m.span()

    def __repr__(self):
        return ('{}(pattern={}, gaps={}, discard_empty={}, flags={})'.format(
                self.__class__.__name__, self._pattern, self._gaps,
                 self._discard_empty, self._flags))


class ParagraphTokenizer(NewRegexpTokenizer):
    """
    Tokenize a string into paragraphs.

    This is accomplished by treating any sentence break character plus any text (ie not a space) followed by a new line
    character as the end of a paragraph.

    """
    def __init__(self):
        NewRegexpTokenizer.__init__(self,
                                    ur'(?<=[\u002E\u2024\uFE52\uFF0E\u0021\u003F][\S]*)\s*\n+',
                                    gaps=True)

