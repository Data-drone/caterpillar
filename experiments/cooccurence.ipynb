{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix, dok_matrix, csr_matrix\n",
    "from collections import defaultdict, OrderedDict\n",
    "from random import randint, choice\n",
    "from pprint import pprint as pp\n",
    "from array import array\n",
    "from itertools import product, combinations\n",
    "from time import perf_counter\n",
    "from copy import copy\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup (Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "import os\n",
    "\n",
    "def mem():\n",
    "    \"\"\" Use psutil to record memory snapshot. \"\"\"\n",
    "    pid = os.getpid()\n",
    "    p = psutil.Process(pid)\n",
    "    rss, vms = p.memory_info()\n",
    "    return vms\n",
    "\n",
    "class Stats:\n",
    "    \"\"\" Context manager for reporting memory change and time cost. \"\"\"\n",
    "    def __enter__(self):\n",
    "        self.m1 = mem()\n",
    "        self.t1 = perf_counter()\n",
    "        \n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.t2 = perf_counter()\n",
    "        self.m2 = mem()\n",
    "        print('\\nChange in memory: ', end='')\n",
    "        print('{:.4g} MB'.format((self.m2 - self.m1) / 1024 / 1024))\n",
    "        print('Time cost (s)   : ', end='')\n",
    "        print('{:.4g} s\\n'.format(self.t2 - self.t1))\n",
    "    \n",
    "# Demo!\n",
    "with Stats():\n",
    "    x = [0]*100000000  # 100M\n",
    "    \n",
    "with Stats():\n",
    "    del x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the list of context blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = [_.strip() for _ in open('/usr/share/dict/words', 'r')]\n",
    "print('Number of unique words: %d\\n' % len(words))\n",
    "words = words[:61000]  # Truncate the list to be more realistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our own hash for bidirectional lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Give word, get index\n",
    "# This is the opposite of `words`: give index, get word\n",
    "wordsd = OrderedDict(zip(words, range(len(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "x = words[1234]\n",
    "print(x)\n",
    "print(wordsd[x])\n",
    "words[32751]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_context_blocks(num_blocks=100000, word_count=(5, 20)):\n",
    "    context_blocks = []\n",
    "    for i in range(num_blocks):\n",
    "        block_size = choice(range(*word_count))\n",
    "        #\n",
    "        # Pretty important that `sorted` is called here. This makes \n",
    "        # combinations stable later.\n",
    "        #\n",
    "        block = sorted(set(choice(words) for i in range(block_size)))\n",
    "        context_blocks.append(block)\n",
    "    return context_blocks\n",
    "             \n",
    "with Stats():\n",
    "    context_blocks = make_context_blocks()\n",
    "    \n",
    "print('Sample blocks:')\n",
    "for b in context_blocks[:5]:\n",
    "    print(' - ', '/'.join(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a version of context_blocks that is only arrays of arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This changes the context blocks, i.e. the list of lists of 5-40 strings, into a two-dimensional array of integers. Each integer is an index into the `idx` hash that was built earlier.\n",
    "\n",
    "The array **is preallocated** for both rows and columns.  Currently we're using a default of 100 for columns.  This easily covers the 5-40 band, obviously.  We use \"-1\" as default, and this is used to know which entries are valid words and which are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_cb_array(context_blocks, max_words_per_block=100):\n",
    "    # Note: values are initialized to -1.  This is to keep track of \n",
    "    # which entries are valid. These will be >=0, and will index into\n",
    "    # the `words` list.\n",
    "    context_blocks_array = np.zeros(\n",
    "        (len(context_blocks), max_words_per_block), \n",
    "        dtype='i4') - 1\n",
    "    for i, block in enumerate(context_blocks):\n",
    "        for j, word in enumerate(block):\n",
    "            # wordsd is a reverse lookup. You give the word, it tells\n",
    "            # you the index in the \"words\" array.\n",
    "            context_blocks_array[i, j] = wordsd[word]\n",
    "    return context_blocks_array\n",
    "\n",
    "context_blocks_array = make_cb_array(context_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Demo\n",
    "print(context_blocks_array[500])\n",
    "for i in context_blocks_array[500]:\n",
    "    if i>-1:\n",
    "        print(words[i], end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive `dict` method.  Dicts inside Dicts\n",
    "\n",
    "(Also, this is all working with strings.  See further down for using naive dicts but with integers everywhere.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def method_dict(context_blocks):\n",
    "    \"\"\"\n",
    "    Given a list of blocks (each containing 5-40 words), build a dict that \n",
    "    itself contain dicts. the inner dict has a count of the number of associations\n",
    "    between the outer key and the inner key.\n",
    "    \"\"\"\n",
    "    d = defaultdict(lambda: defaultdict(int))\n",
    "    for block in context_blocks:\n",
    "        for w1, w2 in combinations(block, 2):\n",
    "            d[w1][w2] += 1\n",
    "    return d\n",
    "\n",
    "with Stats():\n",
    "    d = method_dict(context_blocks)\n",
    "    \n",
    "associations = sum(len(w2s) for w1, w2s in d.items())\n",
    "print('associations: ', associations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `setdefault` all over the place is slower, but really not by much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def method_dict2(context_blocks):\n",
    "    \"\"\"\n",
    "    Given a list of blocks (each containing 5-15 words), build a dict that \n",
    "    itself contain dicts. the inner dict has a count of the number of associations\n",
    "    between the outer key and the inner key.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for block in context_blocks:\n",
    "        for w1, w2 in combinations(block, 2):\n",
    "            d.setdefault(w1, {})\n",
    "            d[w1].setdefault(w2, 0)\n",
    "            d[w1][w2] += 1\n",
    "    return d\n",
    "\n",
    "with Stats():\n",
    "    d2 = method_dict(context_blocks)\n",
    "    \n",
    "associations = sum(len(w2s) for w1, w2s in d2.items())\n",
    "print('associations: ', associations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show a sample of the resulting dict.\n",
    "for i, (w1, w2s) in enumerate(d.items()):\n",
    "    if i > 5:\n",
    "        break\n",
    "    print(w1)\n",
    "    for j, w2 in enumerate(w2s):\n",
    "        if j > 5:\n",
    "            break\n",
    "        print(' '*8, '{:20} {:10}'.format(w2, d[w1][w2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def method_counter(context_blocks):\n",
    "    c = Counter()\n",
    "    for block in context_blocks:\n",
    "        c.update(combinations(block, 2))\n",
    "    return c\n",
    "\n",
    "with Stats():\n",
    "    cnt = method_counter(context_blocks)\n",
    "print('Associations:',len(cnt))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for iter, ((w1, w2), c) in enumerate(cnt.items()):\n",
    "    print('{:15}{:15}{:4}'.format(w1, w2, c))\n",
    "    if iter>5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cython (naive) - Also using dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def method_cython1(list context_blocks):\n",
    "    \"\"\"\n",
    "    Given a list of blocks (each containing 5-40 words), build a dict that \n",
    "    itself contain dicts. the inner dict has a count of the number of associations\n",
    "    between the outer key and the inner key.\n",
    "    \"\"\"\n",
    "    #cdef int n = int(100e6)\n",
    "    #cdef unsigned int[:] w1 = np.zeros(n, dtype='u4') - 1\n",
    "    #cdef unsigned int[:] w2 = np.zeros(n, dtype='u4') - 1\n",
    "    cdef int end = 0, i, j, blen\n",
    "    cdef list block\n",
    "    cdef dict out = {}, inner\n",
    "    cdef str w1, w2\n",
    "    for block in context_blocks:\n",
    "        blen = len(block)\n",
    "        for i in range(blen):\n",
    "            w1 = block[i]\n",
    "            inner = out.get(w1) or {}\n",
    "            for j in range(i+1, blen):\n",
    "                w2 = block[j]\n",
    "                if not w2 in inner:\n",
    "                    inner[w2] = 0\n",
    "                inner[w2] += 1\n",
    "            out[w1] = inner\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with Stats():\n",
    "    d = method_cython1(context_blocks)\n",
    "    \n",
    "associations = sum(len(w2s) for w1, w2s in d.items())\n",
    "print('associations: ', associations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick demo of how to use the integer version of the context blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take on particular block\n",
    "a = context_blocks_array[500]\n",
    "# Take only the assigned words from the block (drop \"-1\"s)\n",
    "b = a[a>-1]\n",
    "print('Words in this block:\\n\\n',b, end='\\n'*2)\n",
    "x = np.zeros(200, dtype='i4')\n",
    "x[5:5+len(b)] = b\n",
    "print('Pair combinations of these words:', end='\\n'*2)\n",
    "for _ in list(combinations(b, 2)):\n",
    "    print(_, end=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools for the numpy work: faster combinations, and `lru_cache`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.misc import comb\n",
    "from itertools import chain\n",
    "from functools import lru_cache\n",
    "\n",
    "# The basic strategy is to build INDICES of \n",
    "# combinations, and then use Numpy's clever\n",
    "# index assignment to generate the actual \n",
    "# combinations arrays.\n",
    "\n",
    "@lru_cache()\n",
    "def comb_index(n, k):\n",
    "    count = comb(n, k, exact=True)\n",
    "    index = np.fromiter(chain.from_iterable(combinations(range(n), k)), \n",
    "                        'i4', count=count*k)\n",
    "    return index.reshape(-1, k)\n",
    "\n",
    "def combb(data):\n",
    "    idx = comb_index(len(data), 2)\n",
    "    return data[idx]\n",
    "\n",
    "# It turns out that 2-combinations are efficiently produced via an upper\n",
    "# triangluar array. Other than that, same as before, we first calculate\n",
    "# the INDICES array, and then pass that into our data to build the\n",
    "# actual list of combinations.\n",
    "\n",
    "@lru_cache()\n",
    "def comb_index_triu(n, k):\n",
    "    return np.array(np.triu_indices(n, 1)).T\n",
    "    \n",
    "def combtriu(data):\n",
    "    idx = comb_index_triu(len(data), 2)\n",
    "    return data[idx]\n",
    "\n",
    "print('Compare the first few elements of each combinations function:', end='\\n\\n')\n",
    "print(combb(b[:3]))\n",
    "print(combtriu(b[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic numpy method.  All arrays, uses fast combinations functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.misc import comb\n",
    "\n",
    "def method_numpy1(context_blocks_array, max_words_per_block=40):\n",
    "    \"\"\"\n",
    "    We create one, very long array (many rows) with 2 columns.  Every time\n",
    "    we add a co-occurence, we simply use a new row to record the two words.\n",
    "    There are some clever tricks inside the sub methods, mostly about how \n",
    "    to work with the combinations efficiently, but basically this pretty \n",
    "    much just records every co-occurence in a pretty dumb way.\n",
    "    \n",
    "    It turns out this is also quite fast.\n",
    "    \n",
    "    Note that we DON'T sum the counts here.  This means that the output \n",
    "    array will have duplicated pairs. IOW there will be multiple rows\n",
    "    with the same two entries.  Afterwards, you will have to sum the\n",
    "    duplicates to determine the co-occurence counts.\n",
    "    \"\"\"\n",
    "    # Pre-allocation of array: WORST CASE\n",
    "    p = comb(max_words_per_block, 2)\n",
    "    n = int(len(context_blocks_array) * p)\n",
    "    print('Worst-case pre-allocation is {:,} entries.'.format(n))\n",
    "    co = np.zeros((n, 2), dtype='i4') - 1\n",
    "    end = 0  # Keep track of position in the allocation array\n",
    "\n",
    "    for block in context_blocks_array:\n",
    "        # Combinations of words in this block. (m, 2) array\n",
    "        new_entries = combtriu(block[block>-1])  \n",
    "        # Copy the new associations directly in\n",
    "        co[end:end+len(new_entries), :] = new_entries\n",
    "        # Move the \"current position\" marker\n",
    "        end += len(new_entries)\n",
    "    \n",
    "    # Return an array of the correct size (truncate)\n",
    "    print('Actual count turned out to be {:,} entries.'.format(end+1))\n",
    "    return co[:end, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del co\n",
    "except:\n",
    "    pass\n",
    "\n",
    "with Stats():\n",
    "    co = method_numpy1(context_blocks_array)\n",
    "    \n",
    "associations = len(co)\n",
    "print('associations: {:,}'.format(associations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use the output?  Use slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Demo of use\n",
    "def top_cooccurences(co, word, most_common_count=10):\n",
    "    \"\"\" \n",
    "        co: one big array (n x 2).  Each entry is an individual co-occurence.\n",
    "        word: A word that you want to find the co-occurences for.\n",
    "        most_common_count: The number of most common co-occurences to return.\n",
    "        \n",
    "    You give a word, this function returns the \n",
    "    other words most strongly associated with\n",
    "    it, along with the counts.\n",
    "    \"\"\"\n",
    "    ix = wordsd[word]\n",
    "    # Find most common pair with \"capivi\"\n",
    "    entries_above = co[co[:,0]==ix]\n",
    "    entries_below = co[co[:,1]==ix]\n",
    "\n",
    "    single_array = np.concatenate((entries_above[:,1], entries_below[:,0]), axis=0)  \n",
    "    idx, counts = np.unique(single_array, return_counts=True)\n",
    "    \n",
    "    other_words = [words[idx[_]] for _ in range(most_common_count)]\n",
    "    return other_words, counts[:most_common_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_cooccurences(co, 'capivi', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the most common associations in the entire result, you would have to build a sparse array to count them.\n",
    "\n",
    "**Note that the act of building the sparse array will also count duplicate entries automatically. It's doing some of our work for us basically.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_sparse(co):\n",
    "    return csr_matrix(\n",
    "            (np.ones(co.shape[0], dtype='u4'), (co[:,0], co[:,1])),\n",
    "            dtype='u4')\n",
    "\n",
    "m = make_sparse(co)\n",
    "m.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can query the top counts across the entire array quite easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All entries with a cooccurence > 2\n",
    "# The two arrays returned are indexes for each dimension.\n",
    "m[m>2].nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're **only interested in rows**, you could also sum the array across the columns and see what comes up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sums = m.sum(axis=1)\n",
    "max_count_index = sums.argmax()\n",
    "max_count_value = sums[max_count_index, 0]\n",
    "print('Word with the biggest count is {} with {}.'.format(max_count_index, max_count_value))\n",
    "print('(That word is {})'.format(words[max_count_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you want to find the top 10 **ROWS**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_rows(m: \"sparse array\", count=10):\n",
    "    sums = m.sum(axis=1).ravel()\n",
    "    #print(sums.shape, sums)\n",
    "    indices = np.argsort(sums, 1)\n",
    "    #print(indices.shape, indices)\n",
    "    indices = indices[0, -count:]\n",
    "    #print(indices.shape, indices)\n",
    "    #print(indices[0, -2])\n",
    "    for ix in range(-1, -count-1, -1):\n",
    "        i = indices[0, ix]\n",
    "        print('{:20} : {:<}'.format(words[i], sums[0,i]))\n",
    "        \n",
    "# Demo\n",
    "print('The top 2:')\n",
    "print('==========')\n",
    "top_rows(m, 2)\n",
    "print()\n",
    "print('The top 10:')\n",
    "print('===========')\n",
    "top_rows(m, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very large test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with Stats():\n",
    "    new_cb = make_context_blocks(num_blocks=int(2e5), word_count=(5,40))\n",
    "    \n",
    "with Stats():\n",
    "    new_cba = make_cb_array(new_cb)\n",
    "\n",
    "print(len(new_cb), len(new_cba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del co2\n",
    "except:\n",
    "    pass\n",
    "\n",
    "with Stats():\n",
    "    co2 = method_numpy1(new_cba)\n",
    "    \n",
    "print('Associations  : ','{0:,}'.format(len(co2)))\n",
    "print('Size of result: {:,.2f} MB'.format(co2.nbytes/1024/1024))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out the top 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with Stats():\n",
    "    m = make_sparse(co2)\n",
    "    \n",
    "print('Size of sparse matrix: {:,.2f} MB'.format(m.data.nbytes/1024/1024))\n",
    "print()\n",
    "print('Top 5 rows (words):')\n",
    "print()\n",
    "    \n",
    "with Stats():\n",
    "    top_rows(m, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# What about `dict` but with ints and our numpy tools?\n",
    "\n",
    "The results are pretty bad, surprisingly so.  Needs more investigation to figure out why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def method_dict_int(context_blocks_array):\n",
    "    \"\"\"\n",
    "    Given a list of blocks (each containing 5-40 words), build a dict that \n",
    "    itself contain dicts. the inner dict has a count of the number of associations\n",
    "    between the outer key and the inner key.\n",
    "    \n",
    "    THIS ONE USES INTEGERS EVERYWHERE.\n",
    "    \"\"\"\n",
    "    d = defaultdict(lambda: defaultdict(int))\n",
    "    for block in context_blocks_array:\n",
    "        for w1, w2 in combtriu(block[block>-1]):\n",
    "            d[w1][w2] += 1\n",
    "    return d\n",
    "\n",
    "with Stats():\n",
    "    d = method_dict_int(context_blocks_array)\n",
    "    \n",
    "associations = sum(sum(w2s.values()) for w1, w2s in d.items())\n",
    "print('associations: ', associations)\n",
    "print(len(context_blocks_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.misc import comb\n",
    "\n",
    "def method_sparse(context_blocks_array, max_words_per_block=40, max_section_length=int(1e7)):\n",
    "    \"\"\"\n",
    "    Series of sparse matrix constructions.\n",
    "    \n",
    "    max_section_length is a setting.  Tweak to trade-off CPU vs RAM.\n",
    "    \"\"\"    \n",
    "    # Max combinations possible in each block\n",
    "    p = comb(max_words_per_block, 2)     \n",
    "    \n",
    "    # Buffers \n",
    "    ones = np.ones(max_section_length, dtype='u2')\n",
    "    co = np.zeros((max_section_length, 2), dtype='u2')\n",
    "    end = 0  # Keep track of position in the allocation array \n",
    "    \n",
    "    # The max number of unique words.  Might need to go up.\n",
    "    # Sets num rows and cols for the output sparse matrix\n",
    "    ns = 2**16  # (65536) \n",
    "    # Output. Stores co-occurrence totals between word pairs.\n",
    "    # The datatype determines the max count possible, and also the \n",
    "    # memory cost of the sparse matrix.  'u2' is quite aggressively\n",
    "    # small. u4 shouldn't be much worse.\n",
    "    m = csr_matrix((ns, ns), dtype='u2')  # \n",
    "    \n",
    "    for block in context_blocks_array:\n",
    "        # Combinations of words in this block.\n",
    "        new_entries = combtriu(block[block>-1])  \n",
    "        #new_entries = combb(block[block>-1]) \n",
    "        # Copy the new associations directly in\n",
    "        co[end:end+len(new_entries), :] = new_entries\n",
    "        # Move the \"current position\" marker\n",
    "        end += len(new_entries)\n",
    "        # Buffer might be full\n",
    "        full = end > max_section_length - p  # Account for next iteration fill-up, worst case\n",
    "        if full:\n",
    "            m += csr_matrix((ones[:end], (co[:end, 0], co[:end, 1])), (ns, ns))\n",
    "            end = 0 # Reset back to start\n",
    "    \n",
    "    if end > 0:\n",
    "        m += csr_matrix((ones[:end], (co[:end, 0], co[:end, 1])), (ns, ns))\n",
    "    return m    \n",
    "    \n",
    "try:\n",
    "    del m\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print('Length of context_blocks_array:',len(context_blocks_array))\n",
    "with Stats():\n",
    "    m = method_sparse(context_blocks_array[:100000], max_section_length=int(1e7))\n",
    "    \n",
    "print('Total co-occurences: {:,}'.format(m.sum()))\n",
    "print('Size of sparse matrix: {:,.2f} MB'.format(m.data.nbytes/1024/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the sparse array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with Stats():\n",
    "    top_rows(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out the big one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del m\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print('Length of context_blocks_array:',len(new_cba))\n",
    "with Stats():\n",
    "    m = method_sparse(new_cba, max_section_length=int(1e6))\n",
    "print('Total co-occurences: {:,}'.format(m.sum()))\n",
    "print('Size of sparse matrix: {:,.2f} MB'.format(m.data.nbytes/1024/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory is great but it's a bit on the slow side.\n",
    "\n",
    "We can increase the buffer size, reducing the number of times a sparse matrix has to be built internally.  Let's search for the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del m\n",
    "    #sleep(0)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print('Length of context_blocks_array:',len(new_cba))\n",
    "for i in range(1,11):\n",
    "    size = int(i*1e7)\n",
    "    print('*********************')\n",
    "    print('Buffer size: {:,}'.format(size))\n",
    "    print('*********************')\n",
    "    with Stats():\n",
    "        m = method_sparse(new_cba, max_section_length=size)\n",
    "    print('Total co-occurences: {:,}'.format(m.sum()))\n",
    "    print('Size of sparse matrix: {:,.2f} MB'.format(m.data.nbytes/1024/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we get our best timings with a buffer length of 3e7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse + Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse option seems to work quite well.  Here we'll try to optimize it using Cython.  The main thing is to remove all interaction with the Python runtime inside the inner loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First make some utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "# cython: language_level = 3\n",
    "cimport cython\n",
    "cimport numpy as np\n",
    "import numpy as np\n",
    "\n",
    "@cython.cdivision(True)\n",
    "cdef inline long comb_count_safe(long n, long k):\n",
    "    cdef long i, prod = 1\n",
    "    for i in range(k):\n",
    "        # The bracketing is super-important. Order of operations matters.\n",
    "        prod = (prod * (n - i)) / (i + 1)\n",
    "        #print(n-i, i+1, prod)\n",
    "    return prod\n",
    "    \n",
    "def comb_cy(int n):\n",
    "    cdef int i, j\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            print(i,j)\n",
    "            \n",
    "cdef inline object comb_cy2(unsigned short n):\n",
    "    cdef int i, j, row = 0\n",
    "    cdef unsigned short[:,:] out = np.zeros((comb_count_safe(n,2), 2), dtype='u2')\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            #print(i,j)\n",
    "            out[row,0] = i\n",
    "            out[row,1] = j\n",
    "            row += 1\n",
    "    return np.array(out)\n",
    "          \n",
    "    \n",
    "def make_lookup_array(unsigned short max_n) -> list:\n",
    "    cdef int i, j, k, n, row = 0\n",
    "    cdef int maxcol = comb_count_safe(max_n, 2)\n",
    "    #cdef list out = [0]\n",
    "    cdef unsigned short[:,:,:] out = np.zeros((max_n+1, maxcol + 1, 2), dtype='u2')\n",
    "    cdef unsigned short[:,:] tmp\n",
    "    for i in range(1, max_n + 1):\n",
    "        n = comb_count_safe(i, 2)\n",
    "        #print(comb_cy2(i))\n",
    "        tmp = comb_cy2(i)\n",
    "        for j in range(n):\n",
    "            for k in range(2):\n",
    "                out[i, j, k] = tmp[j, k]\n",
    "        out[i, maxcol, 0] = n\n",
    "        #out.append(comb_cy2(i))\n",
    "    return np.array(out)\n",
    "    \n",
    "comb_cy(5)\n",
    "print('safe',comb_count_safe(40,2))\n",
    "\n",
    "from scipy.misc import comb\n",
    "print('scipy', comb(40, 2))\n",
    "\n",
    "print(comb_cy2(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = make_lookup_array(5)\n",
    "print(x.shape, x[4, 10, 0])\n",
    "\n",
    "for i, r in enumerate(make_lookup_array(5)):\n",
    "    if i <2:\n",
    "        continue\n",
    "    print()\n",
    "    print('row, ',i)\n",
    "    print('=========')\n",
    "    print()\n",
    "    print(r.shape, x[i, 10, 0], r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(combinations(range(5), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "cimport cython\n",
    "cimport numpy as np\n",
    "import numpy as np\n",
    "from scipy.misc import comb\n",
    "from itertools import chain\n",
    "from functools import lru_cache\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "ctypedef int wtype\n",
    "cdef char* wtype_s = 'i4'\n",
    "\n",
    "@cython.cdivision(True)\n",
    "cdef inline long comb_count_safe(long n, long k):\n",
    "    cdef long i, prod = 1\n",
    "    for i in range(k):\n",
    "        # The bracketing is super-important. Order of operations matters.\n",
    "        prod = (prod * (n - i)) / (i + 1)\n",
    "        #print(n-i, i+1, prod)\n",
    "    return prod\n",
    "            \n",
    "cdef inline object comb_cy2(wtype n):\n",
    "    cdef int i, j, row = 0\n",
    "    cdef wtype[:,:] out = np.zeros((comb_count_safe(n,2), 2), dtype=wtype_s)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            #print(i,j)\n",
    "            out[row,0] = i\n",
    "            out[row,1] = j\n",
    "            row += 1\n",
    "    return np.array(out)\n",
    "          \n",
    "def make_lookup_array(wtype max_n):\n",
    "    cdef int i, j, k, n, row = 0\n",
    "    cdef int maxcol = comb_count_safe(max_n, 2)\n",
    "    cdef wtype[:,:,:] out = np.zeros((max_n+1, maxcol + 1, 2), dtype=wtype_s)\n",
    "    cdef wtype[:,:] tmp\n",
    "    for i in range(1, max_n + 1):\n",
    "        n = comb_count_safe(i, 2)\n",
    "        tmp = comb_cy2(i)\n",
    "        for j in range(n):\n",
    "            for k in range(2):\n",
    "                out[i, j, k] = tmp[j, k]\n",
    "        out[i, maxcol, 0] = n\n",
    "    return np.array(out)\n",
    "\n",
    "def method_sparse_cy2(\n",
    "            int[:, :] context_blocks_array, \n",
    "            int max_words_per_block=40, \n",
    "            int max_section_length=int(1e7)):\n",
    "    \"\"\"\n",
    "    Series of sparse matrix constructions.\n",
    "    \n",
    "    max_section_length is a setting.  Tweak to trade-off CPU vs RAM.\n",
    "    \"\"\"    \n",
    "    \n",
    "    cdef wtype[:,:,:] lookup = make_lookup_array(max_words_per_block)\n",
    "    # Max combinations possible in each block\n",
    "    cdef int p = comb(max_words_per_block, 2)     \n",
    "    \n",
    "    # Buffers \n",
    "    cdef np.ndarray ones = np.ones(max_section_length, dtype=wtype_s)\n",
    "    cdef np.ndarray co = np.zeros((max_section_length, 2), dtype=wtype_s)\n",
    "    cdef wtype[:, :] new_entries = np.zeros((p, 2), dtype=wtype_s)\n",
    "    cdef wtype[:, :] indices = np.zeros((p, 2), dtype=wtype_s)\n",
    "    cdef long end = 0  # Keep track of position in the allocation array \n",
    "    \n",
    "    # The max number of unique words.  Might need to go up.\n",
    "    # Sets num rows and cols for the output sparse matrix\n",
    "    cdef long ns = 2**16  # (65536) \n",
    "    # Output. Stores co-occurrence totals between word pairs.\n",
    "    # The datatype determines the max count possible, and also the \n",
    "    # memory cost of the sparse matrix.  'u2' is quite aggressively\n",
    "    # small. u4 shouldn't be much worse.\n",
    "    m = csr_matrix((ns, ns), dtype=wtype_s)  # \n",
    "\n",
    "    cdef int i, j, k, nn, num_words, n = context_blocks_array.shape[0]\n",
    "    cdef int[:] block\n",
    "    cdef int[:,:] pbuffer = np.zeros((p, 2), dtype=wtype_s)\n",
    "    print('***')\n",
    "    \n",
    "    for block in context_blocks_array:\n",
    "        # Combinations of words in this block.\n",
    "        # Find the number of words in this block\n",
    "        for num_words in range(max_words_per_block):\n",
    "            if block[num_words] == -1:\n",
    "                break    \n",
    "        # Based on the number of words, look up the indices for the \n",
    "        # combinations.\n",
    "        indices = lookup[num_words]\n",
    "        # Now that we have the indices, make the array of actual\n",
    "        # co-occurrences.\n",
    "        for i in range(num_words):\n",
    "            for j in range(2):\n",
    "                new_entries[i, j] = indices[i, j]\n",
    "        #print('new_entries', new_entries.shape, np.array(new_entries))\n",
    "        nn = new_entries[p, 0]\n",
    "        #print('nn',nn)\n",
    "        #return\n",
    "        # Now write the new batch of co-occurrences into the big array.\n",
    "        co[end:end+nn, :] = new_entries[:nn]\n",
    "        # Move the \"current position\" marker\n",
    "        end += nn\n",
    "        # Buffer might be full\n",
    "        full = end > max_section_length - p  # Account for next iteration fill-up, worst case\n",
    "        if full:\n",
    "            m += csr_matrix((ones[:end], (co[:end, 0], co[:end, 1])), (ns, ns))\n",
    "            end = 0 # Reset back to start\n",
    "    \n",
    "    if end > 0:\n",
    "        m += csr_matrix((ones[:end], (co[:end, 0], co[:end, 1])), (ns, ns))\n",
    "    return m    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del m\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print('Length of context_blocks_array:',len(context_blocks_array))\n",
    "print(context_blocks_array.shape, context_blocks_array.dtype)\n",
    "with Stats():\n",
    "    #m = method_sparse_cy1(context_blocks_array, max_section_length=int(1e7))\n",
    "    m = method_sparse_cy2(context_blocks_array)\n",
    "    \n",
    "print('Total co-occurences: {:,}'.format(m.sum()))\n",
    "print('Shape of sparse matrix:', m.shape)\n",
    "print('Size of sparse matrix: {:,.2f} MB'.format(m.data.nbytes/1024/1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del m\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print('Length of context_blocks_array:',len(new_cba))\n",
    "with Stats():\n",
    "    m = method_sparse_cy1(new_cba, max_section_length=int(1e7))\n",
    "    \n",
    "print('Total co-occurences: {:,}'.format(m.sum()))\n",
    "print('Size of sparse matrix: {:,.2f} MB'.format(m.data.nbytes/1024/1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
