{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenisation\n",
    "============\n",
    "As it stands right now, tokenisation in Caterpillar is quite slow. This is mainly due to how the tokenisers are invoked rather than the tokenisers themselves (to an extent anyway).\n",
    "\n",
    "Right now, text is tokenised into paragraphs, then each paragraph is tokenised into frames using the NLTK sentence tokeniser, then each frame is tokenised into words. This involves invoking the sentence and word tokeniser a bunch of times with some serious overhead.\n",
    "\n",
    "An alternative approach would be to invoke each tokeniser once for each body of text. This would required recording the boundaries of tokens for each tokeniser then going through the text once with the list of boundaries from each tokeniser to produce a token stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caterpillar.processing.analysis.analyse import *\n",
    "from caterpillar.processing.analysis.tokenize import *\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets marshall the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = open('/Users/rstuart/Workspace/python/caterpillar/caterpillar/test_resources/moby.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old way first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenised 210736 tokens, 6150 frames\n",
      "CPU times: user 2.45 s, sys: 8.57 ms, total: 2.46 s\n",
      "Wall time: 2.46 s\n"
     ]
    }
   ],
   "source": [
    "def tokenize_old(text):\n",
    "    paragraph_tokenizer = ParagraphTokenizer()\n",
    "    sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    analyser = DefaultAnalyser()\n",
    "    count = 0\n",
    "    frames = []\n",
    "    \n",
    "    for p in paragraph_tokenizer.tokenize(text):\n",
    "        sentences = sentence_tokenizer.tokenize(p.value, realign_boundaries=True)\n",
    "        for i in xrange(0, len(sentences), 2):\n",
    "            frames.append(\" \".join(sentences[i:i+2]))\n",
    "        for s in sentences:\n",
    "            count += len(list(analyser.analyse(s)))\n",
    "    print(\"Tokenised %d tokens, %d frames\" % (count, len(frames)))\n",
    "        \n",
    "%time tokenize_old(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First try at a different approach. The idea here is to insert special markers at the paragraph boundaries (`\\x03`), and sentence boundaries (`\\x02`). Using that information, then mark frame boundaries (`\\x04`). Then tokenise words without crossing one of those boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenised 210702 terms, 5079 frames\n",
      "CPU times: user 2.12 s, sys: 144 ms, total: 2.26 s\n",
      "Wall time: 2.16 s\n"
     ]
    }
   ],
   "source": [
    "def tokenize_new(text):\n",
    "    paragraph_tokenizer = re.compile(\n",
    "        r'\\x02\\\\S{0,4}\\\\s*(?:\\r?\\n)+|\\r?\\n(?:\\r?\\n)+', flags=re.DOTALL | re.UNICODE | re.MULTILINE\n",
    "    )\n",
    "    sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    frame_tokenizer = re.compile(\n",
    "        u'(?:[^\\x03\\x02]*(?:\\x02|(?=\\x03)|$)){1,2}[\\\\s\\x03]*', flags=re.DOTALL | re.UNICODE | re.MULTILINE\n",
    "    )\n",
    "    \n",
    "    # Mark sentences with \\x02\n",
    "    new_text = u\"\\x02\".join([text[start:end] for start, end in sentence_tokenizer.span_tokenize(text)])\n",
    "    # Mark paragraphs with \\x03\n",
    "    new_text = paragraph_tokenizer.sub(u'\\\\g<0>\\x03', new_text)\n",
    "    # Mark frames with \\x04, removing all \\x02 & \\x03\n",
    "    new_text = frame_tokenizer.sub(u'\\\\g<0>\\x04', new_text)\n",
    "    # Remove paragraph and setence markers\n",
    "    new_text = re.sub(u'\\x02|\\x03', '', new_text, flags=re.DOTALL | re.UNICODE | re.MULTILINE)\n",
    "    assert '\\x02' not in new_text and '\\x03' not in new_text\n",
    "    frames = new_text.split('\\x04')\n",
    "    print(\"Tokenised %d terms, %d frames\" % (len(list(DefaultAnalyser().analyse(text))), len(frames)))\n",
    "\n",
    "%time tokenize_new(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is not much of an improvement. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 646 ms, sys: 136 ms, total: 783 ms\n",
      "Wall time: 688 ms\n",
      "CPU times: user 41 ms, sys: 2.05 ms, total: 43.1 ms\n",
      "Wall time: 43.1 ms\n",
      "CPU times: user 25.1 ms, sys: 2.01 ms, total: 27.2 ms\n",
      "Wall time: 27.2 ms\n",
      "CPU times: user 1.4 s, sys: 4.14 ms, total: 1.41 s\n",
      "Wall time: 1.41 s\n",
      "CPU times: user 562 ms, sys: 2.44 ms, total: 564 ms\n",
      "Wall time: 565 ms\n"
     ]
    }
   ],
   "source": [
    "paragraph_tokenizer = re.compile(\n",
    "    r'\\x02\\\\S{0,4}\\\\s*(?:\\r?\\n)+|\\r?\\n(?:\\r?\\n)+', flags=re.DOTALL | re.UNICODE | re.MULTILINE\n",
    ")\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "frame_tokenizer = re.compile(\n",
    "    u'(?:[^\\x03\\x02]*(?:\\x02|(?=\\x03)|$)){1,2}[\\\\s\\x03]*', flags=re.DOTALL | re.UNICODE | re.MULTILINE\n",
    ")\n",
    "word_tokenizer = SimpleWordTokenizer()\n",
    "analyser = DefaultAnalyser()\n",
    "\n",
    "%time sents = u\"\\x02\".join([data[start:end] for start, end in sentence_tokenizer.span_tokenize(data)])\n",
    "%time paras = paragraph_tokenizer.sub(u'\\\\g<0>\\x03', sents)\n",
    "%time frames = frame_tokenizer.sub(u'\\\\g<0>\\x04', paras)\n",
    "%time tokens = list(analyser.analyse(paras))\n",
    "%time tokens = list(word_tokenizer.tokenize(paras))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issues are clearly sentence tokenisation, our analyser and our word tokenizer. There is no low hanging fruit with sentences (although executing it just once rather than for each paragraph is a vast improvement). It really needs a re-write, probably in Cython. But maybe we can do something about the term tokenisation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
