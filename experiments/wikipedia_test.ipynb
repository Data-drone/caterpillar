{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "This is a simplified proof of concept to demonstrate how we could use an analytics style schema for text indexing.\n",
    "\n",
    "The indexing problem is similar, but not identical, to the current processing. This test indexes the full text dump of [English Wikipedia](https://dumps.wikimedia.org/other/cirrussearch/). This is the preprocessed dump file that is used for Wikipedia's own search, so avoids most of the pain of parsing the markup and removing non content information, at the cost of losing some semantic information (like paragraph boundaries). The text field is broken into sentences to simulate the frame workload we currently do.\n",
    "\n",
    "The full Wikipedia dump is a good benchmark for our absolute largest dataset size - if we can handle this then we can handle \n",
    "\n",
    "## Schema\n",
    "\n",
    "This schema is a subset of what we'd actually want - in particular this doesn't yet consider term frequency, term positions or multiple text fields. Adding these and the appropriate indexes to make them usable will add overhead to the insert.\n",
    "\n",
    "The core table is positions - this has one row per term_id, document_id and frame_id - it represents that term_id occured in frame_id of document_id. \n",
    "\n",
    "The actual term representation is in the table vocabulary - one row per unique term occurence. Using the layer of indirection and representing terms by a unique term_id allows filtering terms in the vocabulary table before hitting the much larger positions table. Joins are fast because both tables are ordered by term_id, mapping directly to the SQLite loop join for term searching.\n",
    "\n",
    "### Searching\n",
    "With this layout primitive search operators can be mapped directly to SQL. Writing new style queries is straightforward and doesn't require a Python round trip as in the current layout. Note for example the any_n_search. It should be possible to push scoring to the SQL layer as well, avoiding loading the entire frame result set as in the current scheme.\n",
    "\n",
    "\n",
    "## Dataset notes\n",
    "After processing, the text index is a database of 35GB (documents and frames are not stored in this test, this is representative of the index size alone). \n",
    "\n",
    "Summary statistics:\n",
    "\n",
    "- Number of documents: 5,281,362\n",
    "- Number of frames: 191,172,996\n",
    "- Total vocabulary size: 28,206,884\n",
    "- Total positions rows: 1,613,349,907\n",
    "\n",
    "The vocabulary size is the number of unique terms - only the basic stopword filter is used. Unique terms is particularly large because it includes names, hyphenated forms, misspellings and misformattings, numbers, dates, URL's, case variants (capitalized sentence starts, all caps etc). This has implications for our downstream analysis and modelling - we need to be prepared to deal with messy data.\n",
    "\n",
    "\n",
    "## Performance\n",
    "\n",
    "Total ingestion time for the full dump was ~14 hours. This is promising given the total lack of optimisation to achieve it, and the simple and straightforward schema that results.\n",
    "\n",
    "For small datasets the bottleneck is currently tokenisation - this could be improved by combining parallelisation (documents tokenised in parallel) and improved tokeniser implementation (the document preprocessor approach). The approach here is an underestimate of the tokenisation time, as it doesn't incorporate paragraph tokenization and the consecutive sentence rule. \n",
    "\n",
    "For larger datasets the insert time dominates. Improving this requires:\n",
    "1. Optimised schema/SQLite operations:\n",
    "    - Write the minimum data necessary - just the necessary indexes\n",
    "    - Ensure data locality on writes - (write in primary key order) - might require materialised views rather than covering indexes\n",
    "    - Update to newer versions of SQLite (at a minimum because of improvements in [WAL mode](https://www.sqlite.org/wal.html) from 3.11)\n",
    "    - Tuning the page size?\n",
    "2. Python side optimisation - the insert is a tight loop that might be a candidate for an optimised implementation.\n",
    "3. Write parallelisation through multiple segments and merging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from caterpillar.processing.analysis import stopwords\n",
    "from caterpillar.processing.analysis.analyse import Analyser\n",
    "from caterpillar.processing.analysis.filter import OuterPunctuationFilter, StopFilter, PositionalLowercaseWordFilter, \\\n",
    "    BiGramFilter\n",
    "from caterpillar.processing.analysis.filter import PossessiveContractionFilter\n",
    "from caterpillar.processing.analysis.tokenize import SimpleWordTokenizer\n",
    "\n",
    "\n",
    "class TestAnalyser(Analyser):\n",
    "    _tokenizer = SimpleWordTokenizer(detect_compound_names=False)\n",
    "\n",
    "    def __init__(self, stopword_list=None):\n",
    "        super(TestAnalyser, self).__init__()\n",
    "        if stopword_list is None:\n",
    "            stopword_list = stopwords.ENGLISH_TEST\n",
    "\n",
    "        self._filters = [\n",
    "            OuterPunctuationFilter(leading_allow=['@', '#']),\n",
    "            PossessiveContractionFilter(),\n",
    "            StopFilter(stopword_list, minsize=stopwords.MIN_WORD_SIZE),\n",
    "            PositionalLowercaseWordFilter(0),\n",
    "        ]\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        return self._tokenizer\n",
    "\n",
    "    def get_filters(self):\n",
    "        return self._filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caterpillar.processing.analysis.analyse import DefaultAnalyser, Analyser\n",
    "from caterpillar.processing.analysis.tokenize import SimpleWordTokenizer\n",
    "import nltk\n",
    "import apsw\n",
    "import ujson as json\n",
    "from time import time\n",
    "import gzip\n",
    "import math\n",
    "\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "analyser = TestAnalyser()\n",
    "\n",
    "# Analyser: article_body\n",
    "def analyse(document, stopword_filter=True, shingle=False, paragraph_frames=False):\n",
    "    frame_data = []\n",
    "    if paragraph_frames:\n",
    "        frames = document.split('\\n\\n')\n",
    "        for frame in frames:\n",
    "            tokens = [t.strip() for t in frame.split()]\n",
    "            frame_data.append((frame.strip(), set(tokens)))\n",
    "        \n",
    "    else:\n",
    "        frames = sentence_tokenizer.tokenize(document)\n",
    "        \n",
    "        for frame in frames:\n",
    "            if stopword_filter:\n",
    "                stream = [t.value for t in analyser.analyse(frame) if not t.stopped]\n",
    "            else:\n",
    "                stream = frame.split() #[t.value for t in analyser.analyse(sentence)]\n",
    "\n",
    "            if shingle:\n",
    "                shingles = [' '.join(stream[i:i + 2]) for i in range(len(stream) - 1)]\n",
    "                tokens = set(stream + shingles)\n",
    "            else:\n",
    "                tokens = set(stream)\n",
    "\n",
    "            frame_data.append((frame, tokens))\n",
    "    return document, frame_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'wal',)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise the test database with the appropriate schema\n",
    "# Test frequency and fields later.\n",
    "schema = \"\"\"\n",
    "drop table if exists positions; \n",
    "drop table if exists documents; \n",
    "drop table if exists frames;\n",
    "drop table if exists vocabulary;\n",
    "drop table if exists temp_positions;\n",
    "\n",
    "pragma journal_mode = WAL;\n",
    "pragma page_size = 4096; \n",
    "pragma temp_store = MEMORY;\n",
    "pragma cache_size = 100000;\n",
    "\n",
    "create table positions (\n",
    "    term_id int,\n",
    "    document_id int,\n",
    "    frame_id int,\n",
    "    primary key(term_id, document_id, frame_id))\n",
    "without rowid;\n",
    "\n",
    "create table documents (\n",
    "    document_id integer primary key,\n",
    "    body text);\n",
    "\n",
    "create table frames (\n",
    "    frame_id integer primary key,\n",
    "    body text);\n",
    "    \n",
    "create table vocabulary (\n",
    "    term_id integer primary key,\n",
    "    term text);\n",
    "\n",
    "create index vocab_term_idx on vocabulary(term, term_id);\n",
    "    \n",
    "create temporary table temp_positions (\n",
    "    term text,    \n",
    "    document_id int, \n",
    "    frame_id int);\n",
    "    --,primary key(term, document_id, frame_id);\n",
    "\"\"\"\n",
    "\n",
    "vocab_update = \"\"\"\n",
    "create index insert_positions on temp_positions(term, document_id, frame_id);\n",
    "insert into vocabulary(term) \n",
    "select distinct term\n",
    "from temp_positions\n",
    "where not exists (select 1 from vocabulary v where v.term=temp_positions.term);\n",
    "\"\"\"\n",
    "\n",
    "# Change this to insert the entire positions stream in a single hit\n",
    "positions_insert = \"\"\"\n",
    "insert into positions \n",
    "select term_id, document_id, frame_id\n",
    "from temp_positions\n",
    "inner join vocabulary\n",
    "    on vocabulary.term = temp_positions.term\n",
    "order by term_id;\n",
    "drop index insert_positions\n",
    "\"\"\"\n",
    "\n",
    "test_db = apsw.Connection('wikipedia.db')\n",
    "cursor = test_db.cursor()\n",
    "cursor.execute(schema).fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use sentence or paragraph tokenisation as a test case.\n",
    "\n",
    "def generate_positions(documents, max_documents, max_frames):\n",
    "    frame_id = max_frames + 1\n",
    "    for i, (_, frames) in enumerate(documents):\n",
    "        for _, positions in frames:\n",
    "            for term in positions:\n",
    "                yield (term, max_documents + 1 + i, frame_id)\n",
    "            frame_id += 1\n",
    "\n",
    "def insert_documents(analysed_docs, db_cursor):\n",
    "    try:\n",
    "        db_cursor.execute('begin;')\n",
    "        \n",
    "        max_docs = cursor.execute('select max(document_id) from documents').fetchone()[0]\n",
    "        max_docs = max_docs if max_docs is not None else 0\n",
    "        \n",
    "        max_frames = cursor.execute('select max(frame_id) from frames').fetchone()[0]\n",
    "        max_frames = max_frames if max_frames is not None else 0\n",
    "        \n",
    "        positions = generate_positions(analysed_docs, max_docs, max_frames)\n",
    "        frames = (frame for _, frames in analysed_docs for frame, _ in frames)\n",
    "        numbered_frames = ((i + 1 + max_frames, 'not stored') for i, frame in enumerate(frames))\n",
    "        docs = ((i + 1 + max_docs, 'not stored') for i, (doc, _) in enumerate(analysed_docs))\n",
    "        \n",
    "        # Pipeline the insertion.\n",
    "        db_cursor.executemany('insert into temp_positions values (?, ?, ?);', positions)\n",
    "        db_cursor.executemany('insert into documents values (?, ?)', docs)\n",
    "        db_cursor.executemany('insert into frames values (?, ?)', numbered_frames)\n",
    "\n",
    "        # Now insert new terms into the vocabulary\n",
    "        db_cursor.execute(vocab_update)\n",
    "        # Finally update the positions table\n",
    "        db_cursor.execute(positions_insert)\n",
    "    except Exception as e:\n",
    "        db_cursor.execute('rollback;')\n",
    "        print 'rolling back'\n",
    "        raise e\n",
    "    else:\n",
    "        db_cursor.execute('delete from temp_positions;')\n",
    "        db_cursor.execute('commit;')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wiki_lines = 10562730\n",
    "\n",
    "def add_wikipedia(n_documents=1600, batch_size=200, stopword_filter=True, shingle=False):\n",
    "    # Reset and vaccuum the database\n",
    "    cursor.execute('delete from documents; delete from positions; delete from vocabulary; delete from frames;')\n",
    "    times = [time()]\n",
    "    last_time = times[0]\n",
    "    with gzip.open('enwiki-20161107-cirrussearch-content.json.gz', 'rb') as documents:\n",
    "        print 'starting'\n",
    "        batch = []\n",
    "        batch_docs = 0\n",
    "        for i, line in enumerate(documents):\n",
    "            if i == n_documents:\n",
    "                break\n",
    "\n",
    "            # Batch up the documents, add them all together.\n",
    "            try:\n",
    "                doc = json.loads(line.decode('utf-8'))['text']\n",
    "                batch.append(analyse(doc, stopword_filter=stopword_filter, shingle=shingle, paragraph_frames=False))\n",
    "                batch_docs += 1\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            if batch_docs >= batch_size:\n",
    "                print 'Writing after document {} ({:.2%})'.format(i, i/float(wiki_lines))\n",
    "                times.append(time())\n",
    "                insert_documents(batch, cursor)\n",
    "                batch = []\n",
    "                batch_docs = 0\n",
    "                times.append(time())\n",
    "                print 'done writing, processing batch took {:.1f} s, writing took {:.1f} s'.format(times[-1]-times[-3], times[-1]-times[-2])\n",
    "                print 'cumulative time {:.1f} s'.format(times[-1] - times[0])\n",
    "        else: # Make sure the final batch is written, regardless of size.\n",
    "            insert_documents(batch, cursor)\n",
    "            batch = []\n",
    "            times.append(time())\n",
    "            \n",
    "    times.append(time())\n",
    "    return times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "Writing after document 200001 (1.89%)\n",
      "done writing, processing batch took 658.1 s, writing took 216.3 s\n",
      "cumulative time 658.1 s\n",
      "Writing after document 400001 (3.79%)\n",
      "done writing, processing batch took 678.7 s, writing took 238.5 s\n",
      "cumulative time 1336.8 s\n",
      "Writing after document 600005 (5.68%)\n",
      "done writing, processing batch took 649.4 s, writing took 241.5 s\n",
      "cumulative time 1986.1 s\n",
      "Writing after document 800005 (7.57%)\n",
      "done writing, processing batch took 536.7 s, writing took 205.9 s\n",
      "cumulative time 2522.8 s\n",
      "Writing after document 1000005 (9.47%)\n",
      "done writing, processing batch took 532.2 s, writing took 210.6 s\n",
      "cumulative time 3055.0 s\n",
      "Writing after document 1200005 (11.36%)\n",
      "done writing, processing batch took 612.8 s, writing took 240.2 s\n",
      "cumulative time 3667.8 s\n",
      "Writing after document 1400005 (13.25%)\n",
      "done writing, processing batch took 629.0 s, writing took 261.2 s\n",
      "cumulative time 4296.9 s\n",
      "Writing after document 1600005 (15.15%)\n",
      "done writing, processing batch took 532.0 s, writing took 230.9 s\n",
      "cumulative time 4828.8 s\n",
      "Writing after document 1800005 (17.04%)\n",
      "done writing, processing batch took 451.2 s, writing took 208.4 s\n",
      "cumulative time 5280.1 s\n",
      "Writing after document 2000005 (18.93%)\n",
      "done writing, processing batch took 462.6 s, writing took 212.9 s\n",
      "cumulative time 5742.6 s\n",
      "Writing after document 2200005 (20.83%)\n",
      "done writing, processing batch took 495.1 s, writing took 243.3 s\n",
      "cumulative time 6237.8 s\n",
      "Writing after document 2400005 (22.72%)\n",
      "done writing, processing batch took 472.6 s, writing took 232.9 s\n",
      "cumulative time 6710.4 s\n",
      "Writing after document 2600005 (24.61%)\n",
      "done writing, processing batch took 481.7 s, writing took 248.8 s\n",
      "cumulative time 7192.0 s\n",
      "Writing after document 2800005 (26.51%)\n",
      "done writing, processing batch took 502.2 s, writing took 266.5 s\n",
      "cumulative time 7694.3 s\n",
      "Writing after document 3000005 (28.40%)\n",
      "done writing, processing batch took 509.6 s, writing took 269.9 s\n",
      "cumulative time 8203.9 s\n",
      "Writing after document 3200005 (30.30%)\n",
      "done writing, processing batch took 524.8 s, writing took 295.1 s\n",
      "cumulative time 8728.7 s\n",
      "Writing after document 3400005 (32.19%)\n",
      "done writing, processing batch took 555.0 s, writing took 315.0 s\n",
      "cumulative time 9283.7 s\n",
      "Writing after document 3600005 (34.08%)\n",
      "done writing, processing batch took 552.9 s, writing took 324.8 s\n",
      "cumulative time 9836.6 s\n",
      "Writing after document 3800005 (35.98%)\n",
      "done writing, processing batch took 462.1 s, writing took 230.3 s\n",
      "cumulative time 10298.7 s\n",
      "Writing after document 4000005 (37.87%)\n",
      "done writing, processing batch took 730.3 s, writing took 482.2 s\n",
      "cumulative time 11029.0 s\n",
      "Writing after document 4200005 (39.76%)\n",
      "done writing, processing batch took 603.3 s, writing took 362.1 s\n",
      "cumulative time 11632.3 s\n",
      "Writing after document 4400005 (41.66%)\n",
      "done writing, processing batch took 609.7 s, writing took 362.3 s\n",
      "cumulative time 12242.0 s\n",
      "Writing after document 4600005 (43.55%)\n",
      "done writing, processing batch took 618.6 s, writing took 377.9 s\n",
      "cumulative time 12860.6 s\n",
      "Writing after document 4800005 (45.44%)\n",
      "done writing, processing batch took 614.8 s, writing took 381.7 s\n",
      "cumulative time 13475.4 s\n",
      "Writing after document 5000005 (47.34%)\n",
      "done writing, processing batch took 644.3 s, writing took 398.0 s\n",
      "cumulative time 14119.7 s\n",
      "Writing after document 5200005 (49.23%)\n",
      "done writing, processing batch took 634.7 s, writing took 399.2 s\n",
      "cumulative time 14754.4 s\n",
      "Writing after document 5400005 (51.12%)\n",
      "done writing, processing batch took 660.2 s, writing took 418.4 s\n",
      "cumulative time 15414.6 s\n",
      "Writing after document 5600005 (53.02%)\n",
      "done writing, processing batch took 666.1 s, writing took 428.5 s\n",
      "cumulative time 16080.7 s\n",
      "Writing after document 5800005 (54.91%)\n",
      "done writing, processing batch took 663.6 s, writing took 427.6 s\n",
      "cumulative time 16744.3 s\n",
      "Writing after document 6000005 (56.80%)\n",
      "done writing, processing batch took 668.6 s, writing took 433.1 s\n",
      "cumulative time 17412.9 s\n",
      "Writing after document 6200005 (58.70%)\n",
      "done writing, processing batch took 695.5 s, writing took 449.9 s\n",
      "cumulative time 18108.4 s\n",
      "Writing after document 6400005 (60.59%)\n",
      "done writing, processing batch took 693.2 s, writing took 453.1 s\n",
      "cumulative time 18801.6 s\n",
      "Writing after document 6600005 (62.48%)\n",
      "done writing, processing batch took 749.5 s, writing took 488.0 s\n",
      "cumulative time 19551.1 s\n",
      "Writing after document 6800005 (64.38%)\n",
      "done writing, processing batch took 837.2 s, writing took 530.1 s\n",
      "cumulative time 20388.4 s\n",
      "Writing after document 7000005 (66.27%)\n",
      "done writing, processing batch took 762.7 s, writing took 498.6 s\n",
      "cumulative time 21151.1 s\n",
      "Writing after document 7200005 (68.16%)\n",
      "done writing, processing batch took 811.5 s, writing took 528.3 s\n",
      "cumulative time 21962.6 s\n",
      "Writing after document 7400005 (70.06%)\n",
      "done writing, processing batch took 819.9 s, writing took 544.3 s\n",
      "cumulative time 22782.5 s\n",
      "Writing after document 7600005 (71.95%)\n",
      "done writing, processing batch took 792.5 s, writing took 536.1 s\n",
      "cumulative time 23575.0 s\n",
      "Writing after document 7800005 (73.84%)\n",
      "done writing, processing batch took 928.3 s, writing took 604.0 s\n",
      "cumulative time 24503.3 s\n",
      "Writing after document 8000005 (75.74%)\n",
      "done writing, processing batch took 1171.6 s, writing took 744.1 s\n",
      "cumulative time 25674.9 s\n",
      "Writing after document 8200005 (77.63%)\n",
      "done writing, processing batch took 1212.3 s, writing took 767.2 s\n",
      "cumulative time 26887.2 s\n",
      "Writing after document 8400005 (79.52%)\n",
      "done writing, processing batch took 1474.8 s, writing took 979.4 s\n",
      "cumulative time 28362.0 s\n",
      "Writing after document 8600005 (81.42%)\n",
      "done writing, processing batch took 1588.7 s, writing took 1044.8 s\n",
      "cumulative time 29950.8 s\n",
      "Writing after document 8800005 (83.31%)\n",
      "done writing, processing batch took 1553.2 s, writing took 1046.0 s\n",
      "cumulative time 31504.0 s\n",
      "Writing after document 9000005 (85.21%)\n",
      "done writing, processing batch took 1645.1 s, writing took 1102.9 s\n",
      "cumulative time 33149.1 s\n",
      "Writing after document 9200005 (87.10%)\n",
      "done writing, processing batch took 1634.8 s, writing took 1097.8 s\n",
      "cumulative time 34783.9 s\n",
      "Writing after document 9400005 (88.99%)\n",
      "done writing, processing batch took 1648.7 s, writing took 1104.3 s\n",
      "cumulative time 36432.6 s\n",
      "Writing after document 9600005 (90.89%)\n",
      "done writing, processing batch took 1822.1 s, writing took 1235.1 s\n",
      "cumulative time 38254.6 s\n",
      "Writing after document 9800005 (92.78%)\n",
      "done writing, processing batch took 1917.7 s, writing took 1286.4 s\n",
      "cumulative time 40172.3 s\n",
      "Writing after document 10000005 (94.67%)\n",
      "done writing, processing batch took 1939.4 s, writing took 1303.8 s\n",
      "cumulative time 42111.7 s\n",
      "Writing after document 10200005 (96.57%)\n",
      "done writing, processing batch took 3254.8 s, writing took 2454.6 s\n",
      "cumulative time 45366.5 s\n",
      "Writing after document 10400005 (98.46%)\n",
      "done writing, processing batch took 2672.7 s, writing took 1981.2 s\n",
      "cumulative time 48039.2 s\n",
      "50561.7215071\n"
     ]
    }
   ],
   "source": [
    "times = add_wikipedia(None, batch_size=100000, stopword_filter=True, shingle=False)\n",
    "print times[-1] - times[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position rows: 1613349907\n",
      "Total vocabulary: 28206884\n",
      "Number of frames: 191172996\n",
      "Number of documents: 5281362\n"
     ]
    }
   ],
   "source": [
    "# Report some statistics from whole index:\n",
    "print 'Position rows: {}'.format(cursor.execute('select count(*) from positions;').fetchone()[0])\n",
    "print 'Total vocabulary: {}'.format(cursor.execute('select count(*) from vocabulary;').fetchone()[0])\n",
    "print 'Number of frames: {}'.format(cursor.execute('select count(*) from frames;').fetchone()[0])\n",
    "print 'Number of documents: {}'.format(cursor.execute('select count(*) from documents;').fetchone()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Match any query primitive.\n",
    "search_frame_query = \"\"\"\n",
    "select v.term, frame_id \n",
    "from positions p\n",
    "inner join vocabulary v\n",
    "on p.term_id = v.term_id\n",
    "where v.term in ({})\n",
    "\"\"\"\n",
    "def search_frames(terms):\n",
    "    variable_terms = ', '.join(['?']*len(terms))\n",
    "    this_search = search_frame_query.format(variable_terms)\n",
    "    return list(cursor.execute(this_search, terms))\n",
    "\n",
    "search_document_query = \"\"\"\n",
    "select v.term, document_id, count(*)\n",
    "from positions p\n",
    "inner join vocabulary v\n",
    "on p.term_id = v.term_id\n",
    "where v.term in ({})\n",
    "group by v.term, document_id;\n",
    "\"\"\"\n",
    "\n",
    "def search_documents(terms):\n",
    "    variable_terms = ', '.join(['?']*len(terms))\n",
    "    this_search = search_document_query.format(variable_terms)\n",
    "    return list(cursor.execute(this_search, terms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 113 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit search_frames(['fish', 'dog', 'cat', 'potato'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A simple and a more complex query\n",
    "and_query = \"\"\"\n",
    "select frame_id \n",
    "from positions p\n",
    "inner join vocabulary v\n",
    "on p.term_id = v.term_id\n",
    "where v.term = ?\n",
    "\"\"\"\n",
    "\n",
    "# Match a frame that contains at least n of the specified terms.\n",
    "any_n_query = \"\"\"\n",
    "with any_terms as (\n",
    "select v.term, frame_id \n",
    "from positions p\n",
    "inner join vocabulary v\n",
    "on p.term_id = v.term_id\n",
    "where v.term in ({}))\n",
    "\n",
    "select any_terms.term, any_terms.frame_id\n",
    "from any_terms\n",
    "inner join (\n",
    "    select frame_id, count(term) as overlap\n",
    "    from any_terms\n",
    "    group by frame_id) t_counts\n",
    "on t_counts.frame_id = any_terms.frame_id\n",
    "    and overlap >= ?\n",
    "\"\"\"\n",
    "\n",
    "def and_search(terms):\n",
    "    query = ' intersect '.join([and_query]*len(terms))\n",
    "    return list(cursor.execute(query, terms))\n",
    "\n",
    "def any_n_search(terms, n):\n",
    "    variable_terms = ', '.join(['?']*len(terms))\n",
    "    this_search = any_n_query.format(variable_terms)\n",
    "    return list(cursor.execute(this_search, terms + [n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 26.1 ms per loop\n",
      "10 loops, best of 3: 85.9 ms per loop\n",
      "1 loop, best of 3: 13 s per loop\n"
     ]
    }
   ],
   "source": [
    "# Performance isn't too bad when hitting terms that don't return too many rows. \n",
    "# (At least when there's enough page cache...)\n",
    "%timeit and_search(['apple', 'banana', 'pear', 'carrot'])\n",
    "%timeit any_n_search(['apple', 'banana', 'pear', 'carrot'], 2)\n",
    "\n",
    "# Hits three very common terms - not great performance.\n",
    "%timeit any_n_search(['June', 'July', 'August'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
